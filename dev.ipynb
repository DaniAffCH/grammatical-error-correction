{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jflegDataset import JflegDataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer, GPT2Tokenizer\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tokens = [\"[CLS]\", \"[SEP]\", \"[PAD]\"]\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_tokens(custom_tokens)\n",
    "TRAIN_PATH = \"src/dataset/train.csv\"\n",
    "VAL_PATH = \"src/dataset/eval.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50259,  1026,   481],\n",
       "        [   40,   765,   284],\n",
       "        [50259, 50259,  3666]]), 'attention_mask': tensor([[0, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [0, 0, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.bos_token = \"[CLS]\"\n",
    "tokenizer.eos_token = \"[SEP]\"\n",
    "\n",
    "\n",
    "sentences = [\"It will rain in the\",\n",
    "            \"I want to eat a big bowl of\",\n",
    "            \"My dog is\"]\n",
    "a = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "target = [\"It will\",\n",
    "            \"I want to\",\n",
    "            \"My\"]\n",
    "\n",
    "b = tokenizer(target, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '<|endoftext|>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50257: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50258: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50259: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "\n",
    "    inputs = {key: torch.cat([item[key] for item in inputs], dim=0)\n",
    "              for key in inputs[0].keys()}\n",
    "    targets = [{key: torch.cat([item[key] for item in targets_batch], dim=0)\n",
    "                for key in targets_batch[0].keys()} for targets_batch in zip(*targets)]\n",
    "\n",
    "    return inputs, targets\n",
    "\n",
    "ds_train = JflegDataset(TRAIN_PATH, tokenizer)\n",
    "ds_eval = JflegDataset(VAL_PATH, tokenizer)\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=2)\n",
    "dl_eval = DataLoader(ds_eval, batch_size=2)\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
      "         50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
      "         50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
      "         50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
      "         50259, 50259, 50259, 50259, 50259, 50259, 50259, 50257,   604,    25,\n",
      "          9930,   423,   257,  1263,  2863,   284,  8335,   329,   511,  2003,\n",
      "          1204,   220,   220, 50258],\n",
      "        [50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
      "         50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
      "         50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
      "         50257,   317,  2818,  1672,   286,  6416,  7793,  1134,   373,   618,\n",
      "           347,  1042,   283,   694, 25036,   262,  3850,  3194,   416,   262,\n",
      "          5822,   290,  1908,   340,   284,   262,  4141, 23129, 28283,  6711,\n",
      "           764,   220,   220, 50258]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 4:they have a big chance to prepare for their future life  '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(dl_train)\n",
    "next(it)\n",
    "x,y = next(it)\n",
    "print(x[\"input_ids\"].squeeze())\n",
    "ds_train.decode(x[\"input_ids\"][0].squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "import math\n",
    "\n",
    "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, 0.1)\n",
    "        self.embedding = TokenEmbedding(ntoken, d_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=4,\n",
    "            num_encoder_layers=6,\n",
    "            num_decoder_layers=6,\n",
    "            dim_feedforward=1024,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.head = nn.Linear(d_model, ntoken)\n",
    "        self.sm = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, input: Tensor, target: Tensor, input_mask: Tensor, target_mask: Tensor) -> Tensor:\n",
    "        input_mask = input_mask.bool()\n",
    "        target_mask = target_mask.bool()\n",
    "\n",
    "        src = self.embedding(input) \n",
    "        src = self.pos_encoder(src)\n",
    "\n",
    "        target = self.embedding(target)\n",
    "        target = self.pos_encoder(target)\n",
    "\n",
    "        encoded = self.transformer.encoder(src, src_key_padding_mask=input_mask)\n",
    "        decoded = self.transformer.decoder(target, memory=encoded, tgt_key_padding_mask=target_mask)\n",
    "\n",
    "        out = self.head(decoded)\n",
    "\n",
    "        return self.sm(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.shape=torch.Size([2, 64])\n",
      "target.shape=torch.Size([2, 64])\n",
      "torch.Size([2, 64, 50260])\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel(tokenizer.vocab_size+len(custom_tokens), 768)\n",
    "\n",
    "for x,y in dl_train:\n",
    "    src = x[\"input_ids\"]\n",
    "    target = y[0][\"input_ids\"]\n",
    "    src_mask = x[\"attention_mask\"]\n",
    "    target_mask = y[0][\"attention_mask\"]\n",
    "\n",
    "    print(f\"{src.shape=}\")\n",
    "    print(f\"{target.shape=}\")\n",
    "    out = model(src,target,src_mask,target_mask)\n",
    "    print(out.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TransformerModel.forward() missing 2 required positional arguments: 'input_mask' and 'target_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39mn_classes, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m32\u001b[39m))\n\u001b[1;32m      7\u001b[0m s2s \u001b[38;5;241m=\u001b[39m TransformerModel(n_classes,\u001b[38;5;241m700\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43ms2s\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: TransformerModel.forward() missing 2 required positional arguments: 'input_mask' and 'target_mask'"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_classes = 100\n",
    "\n",
    "    source = torch.randint(low=0, high=n_classes, size=(20, 16))\n",
    "    target = torch.randint(low=0, high=n_classes, size=(20, 32))\n",
    "\n",
    "    s2s = TransformerModel(n_classes,700)\n",
    "\n",
    "    out = s2s(source, target)\n",
    "    print(out.size())\n",
    "    print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
